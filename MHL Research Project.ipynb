{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c38e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#module imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910117e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b48755",
   "metadata": {},
   "source": [
    "## (Activate R Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc34508",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611a2fa",
   "metadata": {},
   "source": [
    "# Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689aac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working director\n",
    "os.chdir('C:/Users/matth/OneDrive/Durham/Masters/Diss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d45ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'plots' directory for saving plots\n",
    "plots_dir = os.path.join(os.getcwd(), \"plots\")\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "# Create 'models' directory for saving models\n",
    "models_dir = os.path.join(os.getcwd(), \"models\")\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958aae7",
   "metadata": {},
   "source": [
    "# R Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(data.table)\n",
    "library(ggplot2)\n",
    "library(ggnewscale)\n",
    "library(tidyverse)\n",
    "library(rgdal)\n",
    "library(raster)\n",
    "library(scoringRules)\n",
    "library(scoringutils)\n",
    "library(raster)\n",
    "\n",
    "options(scipen = 6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e974d",
   "metadata": {},
   "source": [
    "# Elevation Data and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Load in Elevation raster to R and convert to dataframe\n",
    "elevation_r <- raster(\"C:/Users/matth/OneDrive/Durham/Masters/Diss/Data/DEM.tif\")\n",
    "elevimage <- as.data.table(raster::as.data.frame(elevation_r, xy = TRUE, na.rm = TRUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fcf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "###Plot elevation raster\n",
    "ggplot(elevimage) + geom_raster(aes(x = x, y = y, fill = DEM)) + theme_bw() +\n",
    "  scale_fill_viridis_c(option = \"cividis\", begin = 0.2, end = 1, name = 'Elevation, m') + coord_equal() +\n",
    "  theme(legend.justification = c(1, 1), legend.position = c(0.35, 0.99), legend.background=element_blank(),\n",
    "        axis.text.y = element_text(angle = 90, vjust = 0.5, hjust=0.5)) + labs(x = \"Easting (BNG)\", y = \"Northing (BNG)\") +\n",
    "  scale_x_continuous(breaks=seq(250000,500000,by=50000),limits = c(275000,475000)) + scale_y_continuous(breaks=seq(400000,700000,by=50000),limits = c(450000,675000))\n",
    "ggsave(paste0(\"plots/NEelevation.png\"), width = 68, height = 100, units = \"mm\", type = \"cairo\", dpi = 300, scale = 1.375)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9622ae",
   "metadata": {},
   "source": [
    "# Gravity Anomaly Data and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20455ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Load in Gravity Survey data into R and convert to dataframe\n",
    "gravity_r <- raster('C:/Users/matth/OneDrive/Durham/Masters/Diss/Data/Grav.tif')\n",
    "gravimage<-as.data.table(raster::as.data.frame(gravity_r, xy = TRUE, na.rm = TRUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cf430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "###Plot Bouguer Anomaly raster\n",
    "ggplot(gravimage) + geom_raster(aes(x = x, y = y, fill = Grav)) + theme_bw() +\n",
    "  scale_fill_viridis_c(option = \"cividis\", begin = 0.2, end = 1, name = 'Bouguer Anomaly, mGal') + coord_equal() +\n",
    "  theme(legend.justification = c(1, 1), legend.position = c(0.60, 0.99), legend.background=element_blank(),\n",
    "        axis.text.y = element_text(angle = 90, vjust = 0.5, hjust=0.5)) + labs(x = \"Easting (BNG)\", y = \"Northing (BNG)\") +\n",
    "  scale_x_continuous(breaks=seq(250000,500000,by=50000),limits = c(275000,475000)) + scale_y_continuous(breaks=seq(400000,700000,by=50000),limits = c(440000,685000))\n",
    "ggsave(paste0(\"plots/NEGravity.png\"), width = 68, height = 100, units = \"mm\", type = \"cairo\", dpi = 300, scale = 1.375)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff12051",
   "metadata": {},
   "source": [
    "# EVI Data and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Load in EVI data into R and convert to dataframe\n",
    "evi_r <- raster('C:/Users/matth/OneDrive/Durham/Masters/Diss/Data/VI.tif')\n",
    "eviimage<-as.data.table(raster::as.data.frame(evi_r, xy = TRUE, na.rm = TRUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e68748",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "###Plot EVI raster\n",
    "ggplot(eviimage) + geom_raster(aes(x = x, y = y, fill = Band_1/10000)) + theme_bw() +\n",
    "  scale_fill_viridis_c(option = \"cividis\", begin = 0.2, end = 1, name = 'EVI ') + coord_equal() +\n",
    "  theme(legend.justification = c(1, 1), legend.position = c(0.25, 0.99), legend.background=element_blank(),\n",
    "        axis.text.y = element_text(angle = 90, vjust = 0.5, hjust=0.5)) + labs(x = \"Easting (BNG)\", y = \"Northing (BNG)\") +\n",
    "  scale_x_continuous(breaks=seq(250000,500000,by=50000),limits = c(275000,475000)) + scale_y_continuous(breaks=seq(400000,700000,by=50000),limits = c(440000,685000))\n",
    "ggsave(paste0(\"plots/NEEVI.png\"), width = 68, height = 100, units = \"mm\", type = \"cairo\", dpi = 300, scale = 1.375)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d92582",
   "metadata": {},
   "source": [
    "# Mineral Data and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in mineral data\n",
    "Chemical=pd.read_csv('C:/Users/matth/OneDrive/Durham/Masters/Diss/Data/Points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mineral data\n",
    "plt.figure(figsize=(6.8, 10))\n",
    "plt.scatter(Chemical['X_COORD'], Chemical['Y_COORD'], color='black', marker='o', s=1, alpha=0.2)\n",
    "plt.xlim(275000, 475000)\n",
    "plt.ylim(450000, 675000)\n",
    "plt.xlabel('Easting (BNG)')\n",
    "plt.ylabel('Northing (BNG)')\n",
    "plt.yticks(range(450000,700000,50000))\n",
    "plt.xticks(range(300000,500000,50000))\n",
    "plt.title('UK Geochem Plot')\n",
    "plt.legend().set_visible(False)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"plots/UKRSSPoints.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an element name to continue with, out of available names:\n",
    "elem = \"Ni_DCOES\"\n",
    "elemname = \"Nickel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare histograms to see whether a log transformation is appropriate to develop a Gaussian distribution. 0.1 is added to each value to remove 0 values as this would error under logarithmic transformation - this can be removed after analysis.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(Chemical[elem])\n",
    "ax2.hist(np.log(Chemical[elem]+0.1))\n",
    "ax1.title.set_text('Untransformed')\n",
    "ax2.title.set_text('Logarithmic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should a log transformation be used?\n",
    "logtrans = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f092a",
   "metadata": {},
   "source": [
    "# Plot Mineral Data Coloured by Mineral Concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color scale\n",
    "cmap = plt.get_cmap('inferno')\n",
    "if logtrans is True:\n",
    "    color_values = np.log(Chemical[elem] + 0.1)\n",
    "else:\n",
    "    color_values = Chemical[elem]\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(Chemical['X_COORD'], Chemical['Y_COORD'], c=color_values, cmap=cmap, s=5)\n",
    "if logtrans:\n",
    "    norm = plt.Normalize(np.log(Chemical[elem] + 0.1).min(), np.log(Chemical[elem] + 0.1).max())\n",
    "else:\n",
    "    norm = plt.Normalize(Chemical[elem].min(), Chemical[elem].max())\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "cbar.set_label(f\"log({elemname} Concentration)\" if logtrans else f\"{elemname} Concentration\")\n",
    "plt.xlabel('Easting (BNG)')\n",
    "plt.ylabel('Northing (BNG)')\n",
    "plt.yticks(range(450000,700000,50000))\n",
    "plt.xticks(range(300000,500000,50000))\n",
    "plt.title('River Sediment Sample Points')\n",
    "plt.legend().set_visible(False)  # Legend not shown in this case\n",
    "\n",
    "# Show the plot and save it\n",
    "plt.savefig(f'plots/{elemname}_Points.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67143ade",
   "metadata": {},
   "source": [
    "# Extract Location Data, Elevation, Bouguer Anomaly and EVI Values at Each Sample Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i Chemical\n",
    "\n",
    "#Load Mineral data into R, to allow for bilinear sample extraction from rasters\n",
    "Chemical <- as.data.table(Chemical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i elem -o Chemical\n",
    "\n",
    "#Extract Elevation, Bouguer Anomaly and EVI at each sample point, and remove NA's\n",
    "Chemical[, elevation := raster::extract(elevation_r, Chemical[, c(\"X_COORD\", \"Y_COORD\"), with = FALSE], method = \"bilinear\"), ]\n",
    "Chemical[, gravity := raster::extract(gravity_r, Chemical[, c(\"X_COORD\", \"Y_COORD\"), with = FALSE], method = \"bilinear\"), ]\n",
    "Chemical[, evi := raster::extract(evi_r, Chemical[, c(\"X_COORD\", \"Y_COORD\"), with = FALSE], method = \"bilinear\"), ]\n",
    "\n",
    "Chemical <- Chemical[!is.na(Chemical[,get(elem)]) & !is.na(Chemical$elevation) & !is.na(Chemical$gravity) & !is.na(Chemical$evi)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec931de",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb787dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the pixel size and image size of the images for use in the convolutional networks\n",
    "imagedim = 32\n",
    "imageres = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e141dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i imagedim,imageres,Chemical\n",
    "\n",
    "### Extract sample-centred images, for use in feature learning ####\n",
    "#Create array of empty images\n",
    "imgs_elev <- array(dim = c(nrow(Chemical), imagedim, imagedim))\n",
    "imgs_grav <- array(dim = c(nrow(Chemical), imagedim, imagedim))\n",
    "imgs_evi <- array(dim = c(nrow(Chemical), imagedim, imagedim))\n",
    "\n",
    "#Creat image reference for use in loop\n",
    "cells <- as.data.table(expand.grid(x = seq.int(from = imageres/2, by = imageres, length.out = imagedim)-(imageres*imagedim)/2, \n",
    "                                   y = seq.int(from = imageres/2, by = imageres, length.out = imagedim)-(imageres*imagedim)/2))\n",
    "cells[ ,coordx := rep(1:imagedim, imagedim),]\n",
    "cells[ ,coordy := rep(1:imagedim, each = imagedim),]\n",
    "\n",
    "#Loop through each pixel and extract values, for each sample point\n",
    "for(xmeter in unique(cells$coordx)){\n",
    "  for(ymeter in unique(cells$coordy)){\n",
    "    imgs_elev[,xmeter,ymeter] <- extract(elevation_r, method = \"bilinear\",\n",
    "                                    cbind(Chemical$X_COORD + cells[coordx == xmeter & coordy == ymeter]$x, Chemical$Y_COORD + cells[coordx == xmeter & coordy == ymeter]$y)) \n",
    "    imgs_grav[,xmeter,ymeter] <- extract(gravity_r, method = \"bilinear\",\n",
    "                                    cbind(Chemical$X_COORD + cells[coordx == xmeter & coordy == ymeter]$x, Chemical$Y_COORD + cells[coordx == xmeter & coordy == ymeter]$y))\n",
    "    imgs_evi[,xmeter,ymeter] <- extract(evi_r, method = \"bilinear\",\n",
    "                                    cbind(Chemical$X_COORD + cells[coordx == xmeter & coordy == ymeter]$x, Chemical$Y_COORD + cells[coordx == xmeter & coordy == ymeter]$y))\n",
    "  }\n",
    "}\n",
    "\n",
    "#Set NA values to 0\n",
    "imgs_elev[is.na(imgs_elev)] = 0\n",
    "imgs_grav[is.na(imgs_grav)] = 0\n",
    "imgs_evi[is.na(imgs_evi)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o elev_ann,grav_ann,evi_ann\n",
    "\n",
    "#Sample centre the values of each image\n",
    "elev_ann = imgs_elev - Chemical$elevation\n",
    "grav_ann = imgs_grav - Chemical$gravity\n",
    "evi_ann = imgs_evi - Chemical$evi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9bc11",
   "metadata": {},
   "source": [
    "# Location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract easting, northing, elevation, bouguer anomaly and evi as location variables\n",
    "loc = Chemical.loc[:,[\"X_COORD\", \"Y_COORD\", \"elevation\",\"gravity\",'evi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce810e",
   "metadata": {},
   "source": [
    "# Creating Test Train Val Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train, validation and test splits\n",
    "train, testval = train_test_split(np.arange(Chemical.shape[0]), train_size=0.7, random_state = 2023)\n",
    "val, test = train_test_split(testval, train_size=0.5, random_state = 2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split each of the inputs into train, validation and test\n",
    "loc_train = loc.iloc[train]\n",
    "loc_val = loc.iloc[val]\n",
    "loc_test = loc.iloc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_ann_train = elev_ann[train,]\n",
    "elev_ann_val = elev_ann[val,]\n",
    "elev_ann_test = elev_ann[test,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "grav_ann_train = grav_ann[train,]\n",
    "grav_ann_val = grav_ann[val,]\n",
    "grav_ann_test = grav_ann[test,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_ann_train = evi_ann[train,]\n",
    "evi_ann_val = evi_ann[val,]\n",
    "evi_ann_test = evi_ann[test,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149ae35",
   "metadata": {},
   "source": [
    "# Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be7fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise each input by the TRAIN standard deviation, to prevent data leakage\n",
    "elev_sd = elev_ann_train.std()\n",
    "\n",
    "elev_ann_train = elev_ann_train/elev_sd\n",
    "elev_ann_val = elev_ann_val/elev_sd\n",
    "elev_ann_test = elev_ann_test/elev_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grav_sd = grav_ann_train.std()\n",
    "\n",
    "grav_ann_train = grav_ann_train/grav_sd\n",
    "grav_ann_val = grav_ann_val/grav_sd\n",
    "grav_ann_test = grav_ann_test/grav_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c843cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_sd = evi_ann_train.std()\n",
    "\n",
    "evi_ann_train = evi_ann_train/evi_sd\n",
    "evi_ann_val = evi_ann_val/evi_sd\n",
    "evi_ann_test = evi_ann_test/evi_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "locmean = loc_train.mean(axis=0)\n",
    "locsd = loc_train.std(axis=0)\n",
    "\n",
    "loc_train = (loc_train-locmean)/locsd\n",
    "loc_val = (loc_val-locmean)/locsd\n",
    "loc_test = (loc_test-locmean)/locsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8dd92",
   "metadata": {},
   "source": [
    "# Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally combine the independent variables/inputs\n",
    "x_train = [elev_ann_train,grav_ann_train,evi_ann_train,loc_train]\n",
    "x_val = [elev_ann_val,grav_ann_val,evi_ann_val,loc_val]\n",
    "x_test = [elev_ann_test,grav_ann_test,evi_ann_test,loc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dependant variables/expected outputs\n",
    "if(logtrans == True):\n",
    "    y_train = np.log(Chemical.loc[:, elem]+0.1)[train]\n",
    "    y_val = np.log(Chemical.loc[:, elem]+0.1)[val]\n",
    "    y_test = np.log(Chemical.loc[:, elem]+0.1)[test]\n",
    "else:\n",
    "    y_train = Chemical.loc[:, elem][train]\n",
    "    y_val = Chemical.loc[:, elem][val]\n",
    "    y_test = Chemical.loc[:, elem][test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f898af",
   "metadata": {},
   "source": [
    "# Construct the deep learning neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set tuned dropout rate for convolutional and fully connected layers\n",
    "dropratespat=0.65\n",
    "dropratedense=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ec090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional stack for elevation images:\n",
    "conv_input_elev = layers.Input(shape=(imagedim, imagedim, 1), name='conv_input_elev')\n",
    "\n",
    "conv_output_elev = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=3)(conv_input_elev)\n",
    "conv_output_elev = layers.Activation('relu')(conv_output_elev)\n",
    "conv_output_elev = layers.SpatialDropout2D(rate=dropratespat)(conv_output_elev)\n",
    "\n",
    "conv_output_elev = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_elev)\n",
    "conv_output_elev = layers.Activation('relu')(conv_output_elev)\n",
    "conv_output_elev = layers.SpatialDropout2D(rate=dropratespat)(conv_output_elev)\n",
    "\n",
    "conv_output_elev = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_elev)\n",
    "conv_output_elev = layers.Activation('relu')(conv_output_elev)\n",
    "conv_output_elev = layers.SpatialDropout2D(rate=dropratespat)(conv_output_elev)\n",
    "\n",
    "conv_output_elev = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_elev)\n",
    "conv_output_elev = layers.Activation('relu')(conv_output_elev)\n",
    "conv_output_elev = layers.SpatialDropout2D(rate=dropratespat)(conv_output_elev)\n",
    "\n",
    "conv_output_elev = layers.GlobalAveragePooling2D()(conv_output_elev)\n",
    "conv_output_elev = layers.Flatten()(conv_output_elev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3cfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional stack for bouguer anomaly images:\n",
    "conv_input_grav = layers.Input(shape=(imagedim, imagedim, 1), name='conv_input_grav')\n",
    "\n",
    "conv_output_grav = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=3)(conv_input_grav)\n",
    "conv_output_grav = layers.Activation('relu')(conv_output_grav)\n",
    "conv_output_grav = layers.SpatialDropout2D(rate=dropratespat)(conv_output_grav)\n",
    "\n",
    "conv_output_grav = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_grav)\n",
    "conv_output_grav = layers.Activation('relu')(conv_output_grav)\n",
    "conv_output_grav = layers.SpatialDropout2D(rate=dropratespat)(conv_output_grav)\n",
    "\n",
    "conv_output_grav = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_grav)\n",
    "conv_output_grav = layers.Activation('relu')(conv_output_grav)\n",
    "conv_output_grav = layers.SpatialDropout2D(rate=dropratespat)(conv_output_grav)\n",
    "\n",
    "conv_output_grav = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_grav)\n",
    "conv_output_grav = layers.Activation('relu')(conv_output_grav)\n",
    "conv_output_grav = layers.SpatialDropout2D(rate=dropratespat)(conv_output_grav)\n",
    "\n",
    "conv_output_grav = layers.GlobalAveragePooling2D()(conv_output_grav)\n",
    "conv_output_grav = layers.Flatten()(conv_output_grav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional stack for EVI images:\n",
    "conv_input_evi = layers.Input(shape=(imagedim, imagedim, 1), name='conv_input_evi')\n",
    "\n",
    "conv_output_evi = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=3)(conv_input_evi)\n",
    "conv_output_evi = layers.Activation('relu')(conv_output_evi)\n",
    "conv_output_evi = layers.SpatialDropout2D(rate=dropratespat)(conv_output_evi)\n",
    "\n",
    "conv_output_evi = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_evi)\n",
    "conv_output_evi = layers.Activation('relu')(conv_output_evi)\n",
    "conv_output_evi = layers.SpatialDropout2D(rate=dropratespat)(conv_output_evi)\n",
    "\n",
    "conv_output_evi = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_evi)\n",
    "conv_output_evi = layers.Activation('relu')(conv_output_evi)\n",
    "conv_output_evi = layers.SpatialDropout2D(rate=dropratespat)(conv_output_evi)\n",
    "\n",
    "conv_output_evi = layers.Conv2D(128, kernel_size=(3, 3), dilation_rate=1, strides=1)(conv_output_evi)\n",
    "conv_output_evi = layers.Activation('relu')(conv_output_evi)\n",
    "conv_output_evi = layers.SpatialDropout2D(rate=dropratespat)(conv_output_evi)\n",
    "\n",
    "conv_output_evi = layers.GlobalAveragePooling2D()(conv_output_evi)\n",
    "conv_output_evi = layers.Flatten()(conv_output_evi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c76ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary input for locational and auxiliary data:\n",
    "auxiliary_input = layers.Input(shape=(5,), name='aux_input')\n",
    "\n",
    "auxiliary_output = layers.Dense(1920)(auxiliary_input)\n",
    "auxiliary_output = layers.Activation('relu')(auxiliary_output)\n",
    "auxiliary_output = layers.Dropout(rate=dropratedense)(auxiliary_output)\n",
    "auxiliary_output = layers.Flatten()(auxiliary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f046d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concated inputs and final output:\n",
    "concatenated_output = layers.concatenate([conv_output_elev,conv_output_grav,conv_output_evi,auxiliary_output])\n",
    "main_output = layers.Dense(1024)(concatenated_output)\n",
    "main_output = layers.Activation('relu')(main_output)\n",
    "main_output = layers.Dropout(rate=dropratedense)(main_output)\n",
    "main_output = layers.Dense(256)(main_output)\n",
    "main_output = layers.Activation('relu')(main_output)\n",
    "main_output = layers.Dropout(rate=dropratedense)(main_output)\n",
    "main_output = layers.Dense(units=2, activation='linear', name='dist_param')(main_output)\n",
    "main_output = tfp.layers.DistributionLambda(lambda x: tfp.distributions.Normal(\n",
    "    loc=x[..., :1],\n",
    "    #Softplus link function to force the variance to be positive\n",
    "    scale=1e-3 + tf.math.softplus(0.1 * x[..., 1:])\n",
    "))(main_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the final model\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "model = Model(inputs=[conv_input_elev, conv_input_grav, conv_input_evi, auxiliary_input], outputs=main_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c89043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the mode\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42037bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Negative Log Likelihood loss function and set the optimiser\n",
    "def negloglik(y, model):\n",
    "    return -model.log_prob(y)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.002, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(loss=negloglik, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58159d2c",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Batch Size and Number of Epochs\n",
    "batch_size = 2**12\n",
    "epochs = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc41d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=batch_size,epochs=epochs,validation_data=(x_val, y_val),shuffle=True,verbose=2,callbacks=[EarlyStopping(monitor=\"val_loss\", patience=200),ModelCheckpoint(filepath=\"models/modelweights.hdf5\",monitor=\"val_loss\",save_best_only=True,save_weights_only=True)])\n",
    "min_val_loss = min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show minimum validation loss, which will be the model used\n",
    "print(\"Minimum Validation Loss:\", {min_val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16262c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show training history\n",
    "trainhist = pd.DataFrame({'training': history.history['loss'], 'testing': history.history['val_loss']})\n",
    "trainhist['epoch'] = np.arange(1, 601)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=pd.melt(trainhist, id_vars='epoch', value_name='NLL', var_name='dataset'), x='epoch', y='NLL', hue='dataset')\n",
    "plt.ylim(0, np.quantile(trainhist['testing'], 0.999))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Negative Log Likelihood')\n",
    "plt.savefig(f\"plots/Training_{elemname}.png\", dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332a2d3",
   "metadata": {},
   "source": [
    "# Save as Mean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8930a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model Weights from Lowest Validation Loss Model\n",
    "model.load_weights(\"models/modelweights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40101d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile same model to save entire model with weights\n",
    "meanmodel = Model(inputs=[model.input],outputs=[model.get_layer(\"dist_param\").output])\n",
    "\n",
    "meanmodel.save(\"models/meanmodel\")\n",
    "meanmodel = load_model(\"models/meanmodel\")\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c304824",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf09f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of observed and predicted test values using model\n",
    "holdout = pd.DataFrame({'obs': y_test.ravel(), 'preds': meanmodel.predict(x_test)[:,0].ravel()})\n",
    "\n",
    "#Calculate R Squared and RMSE\n",
    "r_squared = np.round(np.corrcoef(holdout['preds'], holdout['obs'])[0, 1] ** 2, 3)\n",
    "rmse = np.round(np.sqrt(np.mean((holdout['preds'] - holdout['obs']) ** 2)), 3)\n",
    "\n",
    "# Plot observed vs predicted\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter\n",
    "sns.scatterplot(data=holdout, x='obs', y='preds', alpha=0.1)\n",
    "plt.plot(holdout['obs'], holdout['obs'], color='red', linestyle='--')\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(f\"R squared = {r_squared}, RMSE = {rmse}\")\n",
    "plt.savefig(f\"plots/mean_holdout_{elemname}.png\", dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print R Squared and RMSE\n",
    "print(f\"R squared = {r_squared}\")\n",
    "print(f\"RMSE = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f556c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of predictions\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "y_pred = meanmodel.predict(x_test, batch_size=batch_size)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dac7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test,y_pred -o mean_testcrps\n",
    "#Calculate CRPS score\n",
    "testcrps <- crps_sample(y_test, drop(replicate(50,y_pred)))\n",
    "mean_testcrps <- mean(testcrps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce23672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show CRPS score\n",
    "print(\"Mean CRPS:\", mean_testcrps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd286f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test,y_pred,logtrans,elem,elemname\n",
    "#Plot Probability Density\n",
    "ggplot() +\n",
    "  stat_density(data = data.frame(x = as.numeric(y_test), data = \"Observed\"), aes(x = x, col = data, linetype = data), geom = \"line\") +\n",
    "  stat_density(data = data.frame(x = as.numeric(replicate(50, y_pred)), data = \"Predicted\"), \n",
    "               aes(x = x, col = data, linetype = data), geom = \"line\") +\n",
    "  scale_x_continuous(limits = c(0,10)) + theme_bw() + theme(legend.justification = c(1,1), legend.position = c(0.95, 0.95), legend.title = element_blank()) +\n",
    "  labs(x = if(logtrans == TRUE){paste0(\"log(\", elem, \")\")}else{paste(elem)},\n",
    "       y = 'Density',\n",
    "       subtitle = paste0(\"CRPS = \", round(mean(testcrps), 2)),\n",
    "       tag = expression(bold(\"b\")))\n",
    "ggsave(paste0(\"plots/distribution_\",elemname,\".png\"), width = 89, height = 89, units = \"mm\", type = \"cairo\", dpi = 300, scale = 1.375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525668cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_pred\n",
    "#Test Quantiles\n",
    "covtest <- melt(as.data.table(data.frame(y = y_test, x = drop(replicate(100, y_pred)))), id.vars = \"y\")\n",
    "paste(\"95% =\", round(nrow(covtest[y < quantile(value, 0.975) & y > quantile(value, 0.025)])/nrow(covtest), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d618c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "paste(\"70% =\", round(nrow(covtest[y < quantile(value, 0.85) & y > quantile(value, 0.15)])/nrow(covtest), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76048b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "paste(\"50% =\", round(nrow(covtest[y < quantile(value, 0.75) & y > quantile(value, 0.25)])/nrow(covtest), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbcca3a",
   "metadata": {},
   "source": [
    "# Mean Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o predgrid\n",
    "###Create Inputs for National Scale maps\n",
    "#Start with auxiliary info\n",
    "gbelevcoarse <- aggregate(elevation_r, fact=1)\n",
    "predgrid <- as.data.table(raster::as.data.frame(gbelevcoarse, xy = TRUE, na.rm = TRUE))[x > 0 & y > 0]\n",
    "setnames(predgrid, \"DEM\", \"elevation\")\n",
    "predgrid$gravity <- extract(gravity_r,method='bilinear', cbind(predgrid$x,predgrid$y))\n",
    "predgrid$evi <- extract(evi_r,method='bilinear', cbind(predgrid$x,predgrid$y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o predimgs_elevann,predimgs_gravann,predimgs_eviann\n",
    "#Next get grid cell-centred images for the convolutional stack\n",
    "predimgs_elev <- array(dim = c(nrow(predgrid), imagedim, imagedim))\n",
    "predimgs_grav <- array(dim = c(nrow(predgrid), imagedim, imagedim))\n",
    "predimgs_evi <- array(dim = c(nrow(predgrid), imagedim, imagedim))\n",
    "\n",
    "for(xmeter in unique(cells$coordx)){\n",
    "  for(ymeter in unique(cells$coordy)){\n",
    "    predimgs_elev[,xmeter,ymeter] <- extract(elevation_r, method = \"bilinear\",\n",
    "                                        cbind(predgrid$x + cells[coordx == xmeter & coordy == ymeter]$x, predgrid$y + cells[coordx == xmeter & coordy == ymeter]$y))\n",
    "    predimgs_grav[,xmeter,ymeter] <- extract(gravity_r, method = \"bilinear\",\n",
    "                                        cbind(predgrid$x + cells[coordx == xmeter & coordy == ymeter]$x, predgrid$y + cells[coordx == xmeter & coordy == ymeter]$y)) \n",
    "    predimgs_evi[,xmeter,ymeter] <- extract(evi_r, method = \"bilinear\",\n",
    "                                        cbind(predgrid$x + cells[coordx == xmeter & coordy == ymeter]$x, predgrid$y + cells[coordx == xmeter & coordy == ymeter]$y)) \n",
    "  }\n",
    "}\n",
    "\n",
    "predimgs_elev[is.na(predimgs_elev)] = 0\n",
    "predimgs_elevann <- predimgs_elev - predgrid$elevation\n",
    "\n",
    "predimgs_grav[is.na(predimgs_grav)] = 0\n",
    "predimgs_gravann <- predimgs_grav - predgrid$gravity\n",
    "\n",
    "predimgs_evi[is.na(predimgs_evi)] = 0\n",
    "predimgs_eviann <- predimgs_evi - predgrid$evi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfca296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the image arrays\n",
    "predimgs_elevann = predimgs_elevann/elev_sd\n",
    "predimgs_gravann = predimgs_gravann/grav_sd\n",
    "predimgs_eviann = predimgs_eviann/evi_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the auxiliary data\n",
    "predloc = predgrid[['x', 'y', 'elevation','gravity','evi']]\n",
    "predloc=predloc.rename(columns={'x':'X_COORD','y':'Y_COORD'})\n",
    "\n",
    "predloc_ann = (predloc-locmean)/locsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions for full study area for both mean and variance (aleatoric uncertainty)\n",
    "pred = meanmodel.predict([predimgs_elevann,predimgs_gravann,predimgs_eviann,predloc_ann], batch_size=batch_size)\n",
    "predgrid[elemname] = pred[:,0]\n",
    "aleatoric = np.array(1e-3 + tf.math.softplus(0.1 * pred[:,1]))\n",
    "\n",
    "#Generate mean raster\n",
    "predraster = predgrid[['x','y',elemname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i predraster,elemname,holdout\n",
    "\n",
    "#Plot the mean map\n",
    "ggplot(predraster) + geom_raster(aes(x = x, y = y, fill = get(elemname)), interpolate = FALSE) + theme_bw() + coord_equal() +\n",
    "  scale_fill_viridis_c(option = \"B\", name = if(logtrans == TRUE){paste0(\"Predicted\\nlog(\",elemname,\" Concentration)\")}else{paste(\"predicted\\n\", elemname, \" Concentration\")}) + labs(x = \"Easting (metres BNG)\", y = \"Northing (metres BNG)\") +\n",
    "  theme(legend.justification = c(1, 1), legend.position = c(0.35, 0.99), legend.background=element_blank(),\n",
    "        axis.text.y = element_text(angle = 90, vjust = 0.5, hjust=0.5)) +\n",
    "  scale_y_continuous(breaks = seq(450000,700000, length.out = 6), labels = paste(seq(450000,700000, length.out = 6)), expand = c(0,0), limits = c(450000, 700000)) +\n",
    "  scale_x_continuous(expand = c(0,0), limits = c(250000, 500000))\n",
    "ggsave(paste0(\"plots/\", elemname, \"_predmap.png\"), width = 128, height = 225, units = \"mm\", type = \"cairo\", dpi = 300, scale = 1.375)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2377c9",
   "metadata": {},
   "source": [
    "# Aleatoric and Epistemic Uncertainty Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove variables to free up memory space\n",
    "import gc\n",
    "del elev_ann, elev_ann_test, elev_ann_train, elev_ann_val, elev_sd, evi_ann, evi_ann_test, evi_ann_train, evi_ann_val, evi_sd, grav_ann, grav_ann_test, grav_ann_train, grav_ann_val, grav_sd, loc, loc_test, loc_train, loc_val, locmean, locsd, x_test, x_train, x_val, y_test, y_train, y_val, y_pred, predraster, predloc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Uncertainty Grid (have to aggregate to double cell size due to memory error)\n",
    "predgrid_uncer = predgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f1dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create list for repeated predictions\n",
    "predictions_list = []\n",
    "\n",
    "#Set Seed\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "#Repeat predictions for full study area, and append to a list\n",
    "for _ in range(100):\n",
    "    predictions = model.predict([predimgs_elevann,predimgs_gravann,predimgs_eviann,predloc_ann], batch_size=batch_size)\n",
    "    predictions_list.append(predictions)\n",
    "\n",
    "# Convert to numpy array\n",
    "predictions_array = np.array(predictions_list)\n",
    "\n",
    "# Calculate the standard deviation for each data point, which we can attribute to epistemic uncertainty\n",
    "epistemic = np.std(predictions_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133998e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add epistemic and aleatoric uncertainties to grid\n",
    "predgrid_uncer['Epistemic'] = epistemic\n",
    "predgrid_uncer['Aleatoric']= aleatoric\n",
    "\n",
    "predgrid_uncer = predgrid_uncer[['x','y','Epistemic','Aleatoric']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i predgrid_uncer,logtrans,elemname\n",
    "#Plot both aleatoric and epistemic uncertainties\n",
    "\n",
    "ggplot(predgrid_uncer) + geom_raster(aes(x = x, y = y, fill = Aleatoric, interpolate = FALSE)) + theme_bw() + coord_equal() +\n",
    "  scale_fill_viridis_c(option = \"B\", name = if(logtrans == TRUE){paste0(\"log(\",elemname,\" Concentration)\\nAleatoric Variance\")}else{paste(elemname, \" Concentration\\nAleatoric Std\")}) + labs(x = \"Easting (metres BNG)\", y = \"Northing (metres BNG)\") +\n",
    "  theme(legend.justification = c(0, 0.5), legend.position = c(0.11, 0.85), legend.background=element_blank(),\n",
    "        axis.text.y = element_text(angle = 90, vjust = 0.5, hjust=0.5))  +\n",
    "  scale_y_continuous(breaks = seq(450000,700000, length.out = 6), labels = paste(seq(450000,700000, length.out = 6)), expand = c(0,0), limits = c(450000, 675000)) +\n",
    "  scale_x_continuous(expand = c(0,0), limits = c(250000, 475000),breaks = seq(250000,500000, length.out = 6), labels = paste(seq(250000,500000, length.out = 6)))\n",
    "ggsave(paste0(\"plots/\", elemname, \"_Aleatoric.png\"), width = 128, height = 225, units = \"mm\", type = \"cairo\", dpi = 300, scale = 1.375)\n",
    "\n",
    "\n",
    "ggplot(predgrid_uncer) + geom_raster(aes(x = x, y = y, fill = Epistemic, interpolate = FALSE)) + theme_bw() + coord_equal() +\n",
    "  scale_fill_viridis_c(option = \"B\", name = if(logtrans == TRUE){paste0(\"log(\",elemname,\" Concentration)\\nEpistemic Std\")}else{paste(elemname, \" Concentration\\nEpistemic Std\")}) + labs(x = \"Easting (metres BNG)\", y = \"Northing (metres BNG)\") +\n",
    "  theme(legend.justification = c(0, 0.5), legend.position = c(0.11, 0.85), legend.background=element_blank(),\n",
    "        axis.text.y = element_text(angle = 90, vjust = 0.5, hjust=0.5))  +\n",
    "  scale_y_continuous(breaks = seq(450000,700000, length.out = 6), labels = paste(seq(450000,700000, length.out = 6)), expand = c(0,0), limits = c(450000, 675000)) +\n",
    "  scale_x_continuous(expand = c(0,0), limits = c(250000, 475000),breaks = seq(250000,500000, length.out = 6), labels = paste(seq(250000,500000, length.out = 6)))\n",
    "ggsave(paste0(\"plots/\", elemname, \"_Epistemic.png\"), width = 128, height = 225, units = \"mm\", type = \"cairo\", dpi = 300, scale = 1.375)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
